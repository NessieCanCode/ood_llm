# Example environment configuration
# Copy this file to `.env.local` and adjust the values as needed.
# It will be automatically loaded during `npm run build` and `npm start`.

# Base URI where the app is mounted
PASSENGER_BASE_URI=/

# URL of an existing llama.cpp server. Leave blank to start via Slurm.
LLAMA_SERVER_URL=

# Port the llama.cpp server listens on
LLAMA_SERVER_PORT=8000

# Slurm options
SLURM_PARTITION=gpu
GPU_TYPE=gpu:1

# Path to llama.cpp server binary and model file
LLAMA_CPP_BIN=/path/to/llama.cpp/server
MODEL=/path/to/models/llama-7b.gguf

# Additional arguments passed to llama.cpp
LLAMA_ARGS=

# Session timeout in seconds
SESSION_TIMEOUT=600
